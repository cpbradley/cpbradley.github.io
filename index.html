<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Christopher Bradley</title>

    <meta name="author" content="Christopher Bradley">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Christopher Bradley
                </p>
                <p>I am a graduate student in the <a href="https://groups.csail.mit.edu/rrg/">Robust Robotics Group</a> at MIT.
                </p>
                <p>
                  I am interested in enabling autonomous robots to act intelligently, particularly in the context of planning hierarchically in the presence of uncertainty. Specifically, I work on developing/learning representations to enable long-horizon decision making for multi-modal problems in partially observable, real-world domains. Before MIT, I studied Mechanical Engineering at the California Institute of Technology.
                </p>
                <p style="text-align:center">
                  <a href="mailto:cbrad@mit.edu">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=WvAc9BkAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <!-- <a href="https://twitter.com/noseworm">Twitter</a> &#38;nbsp;/&#38;nbsp; -->
                  <a href="https://github.com/cpbradley/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/headshot.png">
                  <img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/headshot.png" class="hoverZoomLink">
                </a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <!-- <p>
                  TODO: Add research statement.
                </p> -->
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


    <!-- ArXiv 2024 -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle;text-align:center">
        <img src="images/arxiv24.png" alt="plannin in kitti" height="160" style="border-style: none">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <span class="papertitle">
          Task and Motion Planning in Hierarchical 3D Scene Graphs
        </span>
        <br>
		      <strong>Christopher Bradley*</strong>,
		      <span>Aaron Ray*</span>,
		      <span>Luca Carlone</span>,
	      	<span="">Nicholas Roy</span>
        <br>
          Under Review
        <br>
        <p></p>
        <p>
          An approach for task and motion planning in large environments using Hydra Scene Graphs.
        </p>
        <a href="https://arxiv.org/pdf/2403.08094">Paper</a>
      </td>
    </tr>
    
    <!-- ISER, 2023 -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle;text-align:center">
        <img src="images/iser23.png" alt="Guiding search in TAMP" height="120" style="border-style: solid; border-width: thin">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <span class="papertitle">
          Learning Feasibility and Cost to Guide TAMP
        </span>
        <br>
          <strong>Christopher Bradley</strong>,
	      	<span="">Nicholas Roy</span>
        <br>
          ISER 2023
        <br>
        <p></p>
        <p>
          Accelerating Task and Motion Planning using learned models of feasibility and cost to guide search.
        </p>
        <a href="https://groups.csail.mit.edu/rrg/papers/cbradley_iser_2023.pdf">Paper</a>
      </td>
    </tr>
<!-- 
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle;text-align:center">
       <video  width=100% muted autoplay loop>
          <source src="images/training_sample.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <span class="papertitle">
          Active Learning of Abstract Plan Feasibility
        </span>
        <br>
		      <strong>Michael Noseworthy*</strong>,
		      <span>Caris Moses*</span>,
		      <span>Isaiah Brand*</span>,
	      	<span>Sebastian Castro</span>,
          <span>Leslie Kaelbling</span>,
          <span>Tom&aacute;s Lozano-P&eacute;rez</span>,
          <span>Nicholas Roy</span>
        <br>
          RSS 2021
        <br>
        <p></p>
        <p>
          Efficient online learning of feasility models using ensembles of graph networks.
        </p>
        <a href="https://www.roboticsproceedings.org/rss17/p043.pdf">Paper</a>
        /
        <a href="https://www.youtube.com/watch?v=NM3sv6hzx90">Talk</a>
      </td>
    </tr>

    <!-- ICRA 2020-->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle;text-align:center">
       <video  width=100% muted autoplay loop>
          <source src="images/baxter_prior_fast.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <span class="papertitle">
          Visual Prediction of Priors for Articulated Object Interaction
        </span>
        <br>
          <span>Caris Moses*</span>,
		      <strong>Michael Noseworthy*</strong>,
          <span>Leslie Kaelbling</span>,
          <span>Tom&aacute;s Lozano-P&eacute;rez</span>,
          <span>Nicholas Roy</span>
        <br>
          ICRA 2020
        <br>
        <p></p>
        <p>
          Efficient manipulation of articulated objects using visual priors to infer kinematic parameters.
        </p>
        <a href="http://people.csail.mit.edu/cmm386/publications/vis_prediction_priors.pdf">Paper</a>
        /
        <a href="https://www.youtube.com/watch?v=6ABBn1KpSSU">Talk</a>
        /
        <a href="https://github.com/robustrobotics/honda_cmm">Code</a>
        /
        <a href="https://sites.google.com/view/contextual-prior-prediction">Website</a>
      </td>
    </tr>

    <!-- CORL 2019 -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle;text-align:center">
        <img src="images/corl19.png" alt="Pour Action Demonstration" height="160" style="border-style: none; border-width: thin">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <span class="papertitle">
          Task-Conditioned Variational Autoencoders for Learning Movement Primitives
        </span>
        <br>
		      <strong>Michael Noseworthy</strong>,   
		      <span>Rohan Paul</span>,
          <span>Subhro Roy</span>,
          <span>Daehyung Park</span>,
	      	<span="">Nicholas Roy</span>
        <br>
          CORL 2019
        <br>
        <p></p>
        <p>
          Learning interpretable movement primitives from demonstration.
        </p>
        <a href="https://proceedings.mlr.press/v100/noseworthy20a.html">Paper</a>
      </td>
    </tr>

    <!-- CORL 2019 - Daehyung -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle;text-align:center">
        <img src="images/corl19_park.png" alt="Robot Pipe" height="160" style="border-style: none; border-width: thin">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <span class="papertitle">
          Inferring Task Goals and Constraints using Bayesian Nonparametric Inverse Reinforcement Learning
        </span>
        <br>
          <span>Daehyung Park</span>,
		      <strong>Michael Noseworthy</strong>,   
		      <span>Rohan Paul</span>,
          <span>Subhro Roy</span>,
	      	<span="">Nicholas Roy</span>
        <br>
          CORL 2019
        <br>
        <p></p>
        <p>
          Learning from demonstration in the presence of complex constraints.
        </p>
        <a href="https://proceedings.mlr.press/v100/park20a.html">Paper</a>
      </td>
    </tr>

    <!-- CONLL 2019 -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle;text-align:center">
        <img src="images/conll19.png" alt="Interaction example" height="160" style="border-style: none; border-width: thin">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <span class="papertitle">
          Leveraging Past References for Robust Language Grounding
        </span>
        <br>
          <span>Subhro Roy*</span>,
          <strong>Michael Noseworthy*</strong>,
          <span>Rohan Paul</span>,
          <span>Daehyung Park</span>,
	      	<span="">Nicholas Roy</span>
        <br>
        CoNLL 2019
        <br>
        <p></p>
        <p>
          Natural language grounding in situated and temporally extended contexts.
        </p>
        <a href="https://aclanthology.org/K19-1040/">Paper</a>
      </td>
    </tr>

     <!-- ACL 2017 -->
     <tr>
      <td style="padding:20px;width:25%;vertical-align:middle;text-align:center">
        <img src="images/adem.png" alt="Score correlation" height="160" style="border-style: none; border-width: thin">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <span class="papertitle">
          Towards an Automatic Turing Test: Learning to Evaluate Dialogue Responses
        </span>
        <br>
          <span>Ryan Lowe*</span>,
          <strong>Michael Noseworthy*</strong>,
          <span>Iulian Vlad Serban</span>,
          <span>Nicolas Angelard-Gontier</span>,
	      	<span="">Yoshua Bengio</span>,
          <span="">Joelle Pineau</span>
        <br>
        ACL 2017
        <br>
        <p></p>
        <p>
          Automatic metric for dialogue model response evaluation.
        </p>
        <a href="https://aclanthology.org/P17-1103.pdf">Paper</a>
        /
        <a href="https://github.com/noseworm/ADEM">Code</a>
        /
        <a href="https://vimeo.com/234958888">Talk</a>
      </td>
    </tr>

    <!-- SIGDIAL 2017 -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle;text-align:center">
        <img src="images/sigdial17.png" alt="TSNE Plot" height="160" style="border-style: none; border-width: thin">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <span class="papertitle">
          Predicting Success in Goal-Driven Human-Human Dialogues
        </span>
        <br>
          <strong>Michael Noseworthy</strong>,
	      	<span="">Jackie Chi Kit Cheung</span>,
          <span="">Joelle Pineau</span>
        <br>
        SIGDIAL 2017
        <br>
        <p></p>
        <p>
          Automatic success prediction for task-driven dialogue systems.
        </p>
        <a href="https://aclanthology.org/W17-5531/">Paper</a>
      </td>
    </tr>

    <!-- EMNLP 2016 -->
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle;text-align:center">
        <img src="images/emnlp16.png" alt="Score correlation" height="160" style="border-style: none; border-width: thin">
      </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <span class="papertitle">
          How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation
        </span>
        <br>
          <span>Chia-Wei Liu*</span>,
          <span>Ryan Lowe*</span>,
          <span>Iulian Vlad Serban*</span>,
          <strong>Michael Noseworthy*</strong>,
	      	<span="">Laurent Charlin</span>,
          <span="">Joelle Pineau</span>
        <br>
        EMNLP 2017
        <br>
        <p></p>
        <p>
          A study of how common automatic metrics for evaluating dialogue responses correlate with human judgement.
        </p>
        <a href="https://aclanthology.org/D16-1230/">Paper</a>
        /
        <a href="https://vimeo.com/239251122">Talk</a>
      </td>
    </tr>

      

          </tbody></table> -->

          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle;text-align: center;"><img width="100" src="images/qinai_small.png"></td>
              <td width="75%" valign="center">
                <a href="https://sites.google.com/robot-learning.org/corl2020/attending/inclusioncorl">Inclusion@CoRL Organizer, CoRL 2020</a>
                <br>
                Queer in AI Organizer, RSS 2021
                <br>
                <a href="https://sites.google.com/view/queer-in-ai/corl-2021?authuser=0">Queer in AI Organizer, CoRL 2021</a>
              </td>
            </tr>
            
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:xsmall;">
                  Website template from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
